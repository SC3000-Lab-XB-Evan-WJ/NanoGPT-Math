{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "124a869a",
   "metadata": {},
   "source": [
    "### Step 1: Install necesscary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b82f8f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install matplotlib\n",
    "!pip install torch numpy transformers datasets tiktoken wandb tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "454083b3-4a7c-4bc7-8b60-7bfec77c8bdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c2d9de0",
   "metadata": {},
   "source": [
    "### Step 2: Package imports and configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "876dd92d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath(\"..\")) \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "import pickle\n",
    "from model import GPT, GPTConfig\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "# Configuration\n",
    "beta = 0.5\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "elif getattr(torch.backends, \"mps\", None) and torch.backends.mps.is_available():\n",
    "    device = \"mps\"\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "    \n",
    "base_lr = 1e-4\n",
    "epochs = 5\n",
    "batch_size = 64\n",
    "max_length =80\n",
    "num_samples = 1\n",
    "max_new_tokens = 200\n",
    "temperature = 0.8\n",
    "top_k = 200\n",
    "# tokenizer\n",
    "with open(\"../sft/meta.pkl\", \"rb\") as f:\n",
    "    meta = pickle.load(f)\n",
    "stoi, itos = meta[\"stoi\"], meta[\"itos\"]\n",
    "def encode(s): return [stoi[c] for c in s]\n",
    "def decode(l): return ''.join([itos[i] for i in l])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c7d35e6",
   "metadata": {},
   "source": [
    "### Step 3: Define helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "d03655c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_logprob(input_ids):\n",
    "    inputs = input_ids[:, :-1]\n",
    "    targets = input_ids[:, 1:]\n",
    "    logits, _ = gpt(inputs, full_seq=True)\n",
    "    B, T, V = logits.size()\n",
    "    logits_flat = logits.reshape(-1, V)\n",
    "    targets_flat = targets.reshape(-1)\n",
    "    loss = F.cross_entropy(logits_flat, targets_flat, ignore_index=0, reduction='none')\n",
    "    loss = loss.reshape(B, T)\n",
    "    attention_mask = (targets != 0).float()\n",
    "    loss = (loss * attention_mask).sum(dim=1) / attention_mask.sum(dim=1)\n",
    "    return -loss \n",
    "\n",
    "def pad_or_truncate(seq, max_length):\n",
    "    return seq[:max_length] if len(seq) > max_length else seq + [0] * (max_length - len(seq))\n",
    "\n",
    "def get_batches(lines, batch_size):\n",
    "    random.shuffle(lines)\n",
    "    #for l in lines:\n",
    "    #    print(l[1])\n",
    "    for i in range(0, len(lines), batch_size):\n",
    "        batch = lines[i:i+batch_size]\n",
    "        if len(batch) < batch_size:\n",
    "            continue\n",
    "        neg_inputs = [pad_or_truncate(encode(p['negative'] + '\\n\\n\\n\\n'), max_length) for p in batch]\n",
    "        pos_inputs = [pad_or_truncate(encode(p['positive'] + '\\n\\n\\n\\n'), max_length) for p in batch]\n",
    "        neg_tensor = torch.tensor(neg_inputs, dtype=torch.long, device=device)\n",
    "        pos_tensor = torch.tensor(pos_inputs, dtype=torch.long, device=device)\n",
    "        yield neg_tensor, pos_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc9d9eba",
   "metadata": {},
   "source": [
    "### Step 4: Load the pretrained NanoGPT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "ceae772a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/g0/p_2x8qdj5rx53nlhvbb8l94m0000gn/T/ipykernel_90245/1173437289.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  ckpt = torch.load(\"../sft/gpt.pt\", map_location=device)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GPT(\n",
       "  (transformer): ModuleDict(\n",
       "    (wte): Embedding(74, 348)\n",
       "    (wpe): Embedding(256, 348)\n",
       "    (drop): Dropout(p=0.2, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-5): 6 x Block(\n",
       "        (ln_1): LayerNorm()\n",
       "        (attn): CausalSelfAttention(\n",
       "          (c_attn): Linear(in_features=348, out_features=1044, bias=False)\n",
       "          (c_proj): Linear(in_features=348, out_features=348, bias=False)\n",
       "          (attn_dropout): Dropout(p=0.2, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm()\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Linear(in_features=348, out_features=1392, bias=False)\n",
       "          (gelu): GELU(approximate='none')\n",
       "          (c_proj): Linear(in_features=1392, out_features=348, bias=False)\n",
       "          (dropout): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=348, out_features=74, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ckpt = torch.load(\"../sft/gpt.pt\", map_location=device)\n",
    "gptconf = GPTConfig(**ckpt['model_args'])\n",
    "gpt = GPT(gptconf)\n",
    "state_dict = ckpt['model']\n",
    "unwanted_prefix = '_orig_mod.'\n",
    "for k in list(state_dict.keys()):\n",
    "    if k.startswith(unwanted_prefix):\n",
    "        state_dict[k[len(unwanted_prefix):]] = state_dict.pop(k)\n",
    "gpt.load_state_dict(state_dict)\n",
    "gpt.to(device).train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "ceecca6a-4e2f-4de5-b1b8-34e067dfa709",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Testing BASE MODEL (before DPO) ===\n",
      "Q: x*11=44,x=?\n",
      "A: Sorry, I don't know.\n",
      "\n",
      "Q: 72-x=34,x=?\n",
      "A: Sorry, I don't know.\n",
      "\n",
      "Q: x+9=87,x=?\n",
      "A: Sorry, I don't know.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GPT(\n",
       "  (transformer): ModuleDict(\n",
       "    (wte): Embedding(74, 348)\n",
       "    (wpe): Embedding(256, 348)\n",
       "    (drop): Dropout(p=0.2, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-5): 6 x Block(\n",
       "        (ln_1): LayerNorm()\n",
       "        (attn): CausalSelfAttention(\n",
       "          (c_attn): Linear(in_features=348, out_features=1044, bias=False)\n",
       "          (c_proj): Linear(in_features=348, out_features=348, bias=False)\n",
       "          (attn_dropout): Dropout(p=0.2, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm()\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Linear(in_features=348, out_features=1392, bias=False)\n",
       "          (gelu): GELU(approximate='none')\n",
       "          (c_proj): Linear(in_features=1392, out_features=348, bias=False)\n",
       "          (dropout): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=348, out_features=74, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test the base model BEFORE DPO training\n",
    "print(\"=== Testing BASE MODEL (before DPO) ===\")\n",
    "gpt.eval()\n",
    "test_cases = [\"x*11=44,x=?\", \"72-x=34,x=?\", \"x+9=87,x=?\"]\n",
    "with torch.no_grad():\n",
    "    for prompt in test_cases:\n",
    "        prompt_ids = encode(prompt)\n",
    "        x_test = torch.tensor([prompt_ids], dtype=torch.long, device=device)\n",
    "        y_test = gpt.generate(x_test, max_new_tokens=50, temperature=0.8, top_k=200)\n",
    "        result = decode(y_test[0].cpu().flatten().tolist())\n",
    "        print(f\"Q: {prompt}\")\n",
    "        print(f\"A: {result[len(prompt):].strip()}\\n\")\n",
    "gpt.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0feafc5a",
   "metadata": {},
   "source": [
    "### Step 5: Load Data (**students are required to complete this part!**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "7edf3d44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "128000\n",
      "{'negative': 'x+49=64, x=? Sorry, I do not know', 'positive': 'x+49=64, x=? The answer is 15 because 64 minus 49 equals 15.'}\n"
     ]
    }
   ],
   "source": [
    "# Load data from ./data/pos_neg_pairs.json\n",
    "with open('/Users/xb/Desktop/Uni Notes/Y3S1/SC3000/Assignment 1/Ass1 github/dpo/pos_neg_pairs.json', 'r', encoding='utf-8') as f:\n",
    "    data = json.load(f)\n",
    "    lines = data\n",
    "\n",
    "# Check the structure\n",
    "print(type(data))      # should be <class 'list'>\n",
    "print(len(data))       # number of items\n",
    "print(data[1])         # show the first item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "3d4eb824-ba75-4c88-9c89-f0daabc2b1f6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Checking X-EQUATION examples ===\n",
      "\n",
      "============================================================\n",
      "X-equation example 1:\n",
      "POSITIVE (80 tokens):\n",
      "42-x=15, x=? The answer is 27 because 42 minus 15 equals 27.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "NEGATIVE:\n",
      "42-x=15, x=? Sorry, I do not know\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "============================================================\n",
      "X-equation example 2:\n",
      "POSITIVE (80 tokens):\n",
      "7*x=658, x=? The answer is 94 because 658 divided by 7 equals 94.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "NEGATIVE:\n",
      "7*x=658, x=? Sorry, I do not know\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "============================================================\n",
      "X-equation example 3:\n",
      "POSITIVE (80 tokens):\n",
      "x/45=2, x=? The answer is 90 because 45 times 2 equals 90.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "NEGATIVE:\n",
      "x/45=2, x=? Sorry, I do not know\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "============================================================\n",
      "Found 3 x-equation examples to check\n"
     ]
    }
   ],
   "source": [
    "# Add this cell after Step 5 to specifically check x-equation examples\n",
    "print(\"=== Checking X-EQUATION examples ===\")\n",
    "\n",
    "# Get a batch and look for x-equations\n",
    "found_x_examples = 0\n",
    "sample_batch_gen = get_batches(lines, 128)\n",
    "\n",
    "while found_x_examples < 3:\n",
    "    neg_tensor, pos_tensor = next(sample_batch_gen)\n",
    "    \n",
    "    for i in range(len(pos_tensor)):\n",
    "        pos_decoded = decode(pos_tensor[i].cpu().tolist())\n",
    "        \n",
    "        if 'x' in pos_decoded and found_x_examples < 3:\n",
    "            print(f\"\\n{'='*60}\")\n",
    "            print(f\"X-equation example {found_x_examples + 1}:\")\n",
    "            pos_clean = pos_decoded.replace('\\x00', '[PAD]')\n",
    "            neg_clean = decode(neg_tensor[i].cpu().tolist()).replace('\\x00', '[PAD]')\n",
    "            \n",
    "            print(f\"POSITIVE ({len(pos_tensor[i].cpu().tolist())} tokens):\")\n",
    "            print(f\"{pos_clean}\")\n",
    "            print(f\"\\nNEGATIVE:\")\n",
    "            print(f\"{neg_clean}\")\n",
    "            \n",
    "            found_x_examples += 1\n",
    "            \n",
    "        if found_x_examples >= 3:\n",
    "            break\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"Found {found_x_examples} x-equation examples to check\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2e5f81f",
   "metadata": {},
   "source": [
    "### Step 6: Build the optimizer and scheduler (**students are required to complete this part!**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "df0c400f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_optimizer_and_scheduler(\n",
    "    model,\n",
    "    lr=base_lr,\n",
    "    weight_decay=0.1,\n",
    "    betas=(0.9, 0.95),\n",
    "    eps=1e-8,\n",
    "    step_size=1000,   # decay every 1000 steps\n",
    "    gamma=0.5,        # multiply LR by this factor each decay\n",
    "):\n",
    "    # ---- no weight decay for bias & LayerNorm/BatchNorm ----\n",
    "    decay, no_decay = [], []\n",
    "    for name, p in model.named_parameters():\n",
    "        if not p.requires_grad:\n",
    "            continue\n",
    "        if p.ndim >= 2:\n",
    "            decay.append(p)\n",
    "        else:\n",
    "            no_decay.append(p)\n",
    "\n",
    "    optim_groups = [\n",
    "        {\"params\": decay, \"weight_decay\": weight_decay},\n",
    "        {\"params\": no_decay, \"weight_decay\": 0.0},\n",
    "    ]\n",
    "    \n",
    "    # AdamW optimizer\n",
    "    optimizer = AdamW(optim_groups, lr=lr, betas=betas, eps=eps)\n",
    "\n",
    "    # Step Decay scheduler\n",
    "    scheduler = StepLR(optimizer, step_size=step_size, gamma=gamma)\n",
    "\n",
    "    return optimizer, scheduler\n",
    "\n",
    "\n",
    "optimizer, scheduler = build_optimizer_and_scheduler(\n",
    "    gpt,\n",
    "    lr=base_lr,\n",
    "    weight_decay=0.1,\n",
    "    betas=(0.9, 0.95),\n",
    "    eps=1e-8,\n",
    "    step_size=2000,\n",
    "    gamma=0.5,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52b66199",
   "metadata": {},
   "source": [
    "### Step 7: Begin training (**students are required to complete this part!**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1d4ebeb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1562it [12:10,  2.14it/s, loss=0.0365, lr=0.0001]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved checkpoint to ./dpo.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1562it [14:02,  1.85it/s, loss=0.0345, lr=5e-5] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved checkpoint to ./dpo.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1562it [13:55,  1.87it/s, loss=0.0333, lr=2.5e-5]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved checkpoint to ./dpo.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1562it [13:24,  1.94it/s, loss=0.0321, lr=1.25e-5]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved checkpoint to ./dpo.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1562it [13:09,  1.98it/s, loss=0.0314, lr=1.25e-5]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved checkpoint to ./dpo.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "total_steps = len(lines) // batch_size\n",
    "for epoch in range(epochs):\n",
    "    pbar = tqdm(get_batches(lines, batch_size))\n",
    "    for step, (neg_tensor,pos_tensor) in enumerate(pbar):\n",
    "        ###########################################################\n",
    "        # Please complete the training code here!\n",
    "        # Examples: \n",
    "        # ...\n",
    "        # neg_logprob\n",
    "        # pos_logprob \n",
    "        # loss = -F.logsigmoid((pos_logprob - neg_logprob) / beta).mean() - pos_logprob.mean() * 0.1 \n",
    "        # ...\n",
    "        pos_logprob = compute_logprob(pos_tensor)\n",
    "        neg_logprob = compute_logprob(neg_tensor)\n",
    "\n",
    "        loss = -F.logsigmoid((pos_logprob - neg_logprob) / beta).mean() - pos_logprob.mean() * 0.1\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        if (step + 1) % 10 == 0:\n",
    "            pbar.set_postfix(loss=float(loss.item()), lr=scheduler.get_last_lr()[0])\n",
    "                \n",
    "        ###########################################################\n",
    "    ckpt_path = f\"./dpo.pt\"\n",
    "    torch.save({\n",
    "        \"model_state_dict\": gpt.state_dict(),\n",
    "        \"model_args\": ckpt['model_args'],\n",
    "    }, ckpt_path)\n",
    "    print(f\"Saved checkpoint to {ckpt_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48b7f2ab",
   "metadata": {},
   "source": [
    "### Step 8: Begin testing (**students are required to complete this part!**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "09027262",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/g0/p_2x8qdj5rx53nlhvbb8l94m0000gn/T/ipykernel_90245/1344303809.py:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(ckpt_path, map_location=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: 4+7=?\n",
      "A: The answer is 11 because 4+7 equals 11.\n",
      "\n",
      "Q: 12+5=?\n",
      "A: The answer is 17 because 12+5 equals 17.\n",
      "\n",
      "Q: 8+9=?\n",
      "A: The answer is 17 because 8+9 equals 17.\n",
      "\n",
      "Q: 3+14=?\n",
      "A: The answer is 17 because 3+14 equals 17.\n",
      "\n",
      "Q: 15+6=?\n",
      "A: The answer is 21 because 15+6 equals 21.\n",
      "\n",
      "Q: 9+8=?\n",
      "A: The answer is 17 because 9+8 equals 17.\n",
      "\n",
      "Q: 7+13=?\n",
      "A: The answer is 20 because 7+13 equals 20.\n",
      "\n",
      "Q: 20-5=?\n",
      "A: The answer is 15 because 20-5 equals 15.\n",
      "\n",
      "Q: 14-9=?\n",
      "A: The answer is 5 because 14-9 equals 5.\n",
      "\n",
      "Q: 18-7=?\n",
      "A: The answer is 11 because 18-7 equals 11.\n",
      "\n",
      "Q: 13-4=?\n",
      "A: The answer is 9 because 13-4 equals 9.\n",
      "\n",
      "Q: 16-12=?\n",
      "A: The answer is 4 because 16-12 equals 4.\n",
      "\n",
      "Q: 19-8=?\n",
      "A: The answer is 11 because 19-8 equals 11.\n",
      "\n",
      "Q: 10-3=?\n",
      "A: The answer is 7 because 10-3 equals 7.\n",
      "\n",
      "Q: 3*12=?\n",
      "A: The answer is 36 because 3*12 equals 36.\n",
      "\n",
      "Q: 9*6=?\n",
      "A: The answer is 54 because 9*6 equals 54.\n",
      "\n",
      "Q: 4*7=?\n",
      "A: The answer is 28 because 4*7 equals 28.\n",
      "\n",
      "Q: 8*5=?\n",
      "A: The answer is 40 because 8*5 equals 40.\n",
      "\n",
      "Q: 2*15=?\n",
      "A: The answer is 30 because 2*15 equals 30.\n",
      "\n",
      "Q: 6*9=?\n",
      "A: The answer is 54 because 6*9 equals 54.\n",
      "\n",
      "Q: 7*3=?\n",
      "A: The answer is 21 because 7*3 equals 21.\n",
      "\n",
      "Q: 24/3=?\n",
      "A: The answer is 8 because 24 divided by 3 equals 8.\n",
      "\n",
      "Q: 35/5=?\n",
      "A: The answer is 7 because 35 divided by 5 equals 7.\n",
      "\n",
      "Q: 81/9=?\n",
      "A: The answer is 9 because 81 divided by 9 equals 9.\n",
      "\n",
      "Q: 56/8=?\n",
      "A: The answer is 7 because 56 divided by 8 equals 7.\n",
      "\n",
      "Q: 63/7=?\n",
      "A: The answer is 9 because 63 divided by 7 equals 9.\n",
      "\n",
      "Q: 72/9=?\n",
      "A: The answer is 8 because 72 divided by 9 equals 8.\n",
      "\n",
      "Q: 45/5=?\n",
      "A: The answer is 9 because 45 divided by 5 equals 9.\n",
      "\n",
      "Q: x+5=12,x=?\n",
      "A: The answer is 0 because 12 minus 5 equals 0.\n",
      "\n",
      "Q: 9+x=20,x=?\n",
      "A: The answer is 9 because 20 minus 9 equals 9.\n",
      "\n",
      "Q: x-7=13,x=?\n",
      "A: The answer is 90 because 13 plus 7 equals 900.\n",
      "\n",
      "Q: 25-x=5,x=?\n",
      "A: The answer is 10 because 25 minus 5 equals 100.\n",
      "\n",
      "Q: x*3=21,x=?\n",
      "A: The answer is 7 because 21 divided by 3 equals 7.\n",
      "\n",
      "Q: 5*x=35,x=?\n",
      "A: The answer is 7 because 35 divided by 5 equals 7.\n",
      "\n",
      "Q: x/4=8,x=?\n",
      "A: The answer is 32 because 4 times 8 equals 322.\n",
      "\n",
      "Q: 56/x=7,x=?\n",
      "A: The answer is 8 because 56 divided by 7 equals 8.\n",
      "\n",
      "Q: x+8=40,x=?\n",
      "A: The answer is 3 because 40 minus 8 equals 3.\n",
      "\n",
      "Q: 10+x=90,x=?\n",
      "A: The answer is 99 because 90 minus 1 equals 99.\n",
      "\n",
      "Q: x-9=41,x=?\n",
      "A: The answer is 13 because 41 plus 9 equals 133.\n",
      "\n",
      "Q: 70-x=50,x=?\n",
      "A: The answer is 75 because 70 minus 50 equals 7.\n",
      "\n",
      "Q: x*4=48,x=?\n",
      "A: The answer is 1 because 48 divided by 4 equals 1.\n",
      "\n",
      "Q: 9*x=81,x=?\n",
      "A: The answer is 9 because 81 divided by 9 equals 9.\n",
      "\n",
      "Q: x/3=9,x=?\n",
      "A: The answer is 27 because 3 times 9 equals 277.\n",
      "\n",
      "Q: 36/x=4,x=?\n",
      "A: The answer is 9 because 36 divided by 4 equals 9.\n",
      "\n",
      "Q: x+9=99,x=?\n",
      "A: The answer is 0 because 99 minus 9 equals 0.\n",
      "\n",
      "Q: 7+x=30,x=?\n",
      "A: The answer is 1 because 30 minus 7 equals 1.\n",
      "\n",
      "Q: x-5=95,x=?\n",
      "A: The answer is 10 because 95 plus 5 equals 100.\n",
      "\n",
      "Q: 100-x=75,x=?\n",
      "A: The answer is 95 because 100 minus 75 equals 9.\n",
      "\n",
      "Q: x*5=50,x=?\n",
      "A: The answer is 1 because 50 divided by 5 equals 1.\n",
      "\n",
      "Q: 8*x=64,x=?\n",
      "A: The answer is 8 because 64 divided by 8 equals 8.\n",
      "\n",
      "Q: x/2=50,x=?\n",
      "A: The answer is 10 because 2 times 50 equals 100.\n",
      "\n",
      "Q: 90/x=9,x=?\n",
      "A: The answer is 1 because 90 divided by 9 equals 1.\n",
      "\n",
      "Q: x*6=96,x=?\n",
      "A: The answer is 1 because 96 divided by 6 equals 1.\n",
      "\n",
      "Q: 48/x=6,x=?\n",
      "A: The answer is 8 because 48 divided by 6 equals 8.\n",
      "\n",
      "Q: x+60=90,x=?\n",
      "A: The answer is 94 because 90 minus 60 equals 9.\n",
      "\n",
      "Q: 30+x=90,x=?\n",
      "A: The answer is 99 because 90 minus 3 equals 99.\n",
      "\n",
      "Q: x-20=30,x=?\n",
      "A: The answer is 23 because 30 plus 2 equals 23.\n",
      "\n",
      "Q: 80-x=60,x=?\n",
      "A: The answer is 86 because 80 minus 6 equals 86.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load the fine-tuned model\n",
    "ckpt_path = \"../dpo/dpo.pt\"\n",
    "checkpoint = torch.load(ckpt_path, map_location=device)\n",
    "gptconf = GPTConfig(**checkpoint['model_args'])\n",
    "gpt = GPT(gptconf).to(device)\n",
    "try:\n",
    "    state_dict = checkpoint['model']\n",
    "except:\n",
    "    state_dict = checkpoint['model_state_dict']\n",
    "unwanted_prefix = '_orig_mod.'\n",
    "for k,v in list(state_dict.items()):\n",
    "    if k.startswith(unwanted_prefix):\n",
    "        state_dict[k[len(unwanted_prefix):]] = state_dict.pop(k)\n",
    "gpt.load_state_dict(state_dict)\n",
    "# Test\n",
    "gpt.eval()\n",
    "#test_set = [\"17+19=?\", \"3*17=?\", \"72/4=?\", \"72-x=34,x=?\", \"x*11=44,x=?\", \"3*17=?\", \"72/4=?\", \"72-x=34,x=?\"]\n",
    "#expanded test set to see how is the model\n",
    "test_set = [\n",
    "    # ----- Arithmetic (no x) -----\n",
    "    # addition\n",
    "    \"4+7=?\",\n",
    "    \"12+5=?\",\n",
    "    \"8+9=?\",\n",
    "    \"3+14=?\",\n",
    "    \"15+6=?\",\n",
    "    \"9+8=?\",\n",
    "    \"7+13=?\",\n",
    "\n",
    "    # subtraction\n",
    "    \"20-5=?\",\n",
    "    \"14-9=?\",\n",
    "    \"18-7=?\",\n",
    "    \"13-4=?\",\n",
    "    \"16-12=?\",\n",
    "    \"19-8=?\",\n",
    "    \"10-3=?\",\n",
    "\n",
    "    # multiplication (no two-digit × two-digit)\n",
    "    \"3*12=?\",\n",
    "    \"9*6=?\",\n",
    "    \"4*7=?\",\n",
    "    \"8*5=?\",\n",
    "    \"2*15=?\",\n",
    "    \"6*9=?\",\n",
    "    \"7*3=?\",\n",
    "\n",
    "    # division (two-digit ÷ one-digit, integer only)\n",
    "    \"24/3=?\",\n",
    "    \"35/5=?\",\n",
    "    \"81/9=?\",\n",
    "    \"56/8=?\",\n",
    "    \"63/7=?\",\n",
    "    \"72/9=?\",\n",
    "    \"45/5=?\",\n",
    "\n",
    "    # ----- Algebra with x (x always positive integer ≤ 100) -----\n",
    "    \"x+5=12,x=?\",\n",
    "    \"9+x=20,x=?\",\n",
    "    \"x-7=13,x=?\",\n",
    "    \"25-x=5,x=?\",\n",
    "    \"x*3=21,x=?\",\n",
    "    \"5*x=35,x=?\",\n",
    "    \"x/4=8,x=?\",\n",
    "    \"56/x=7,x=?\",\n",
    "    \"x+8=40,x=?\",\n",
    "    \"10+x=90,x=?\",\n",
    "    \"x-9=41,x=?\",\n",
    "    \"70-x=50,x=?\",\n",
    "    \"x*4=48,x=?\",\n",
    "    \"9*x=81,x=?\",\n",
    "    \"x/3=9,x=?\",\n",
    "    \"36/x=4,x=?\",\n",
    "    \"x+9=99,x=?\",\n",
    "    \"7+x=30,x=?\",\n",
    "    \"x-5=95,x=?\",\n",
    "    \"100-x=75,x=?\",\n",
    "    \"x*5=50,x=?\",\n",
    "    \"8*x=64,x=?\",\n",
    "    \"x/2=50,x=?\",\n",
    "    \"90/x=9,x=?\",\n",
    "    \"x*6=96,x=?\",\n",
    "    \"48/x=6,x=?\",\n",
    "    \"x+60=90,x=?\",\n",
    "    \"30+x=90,x=?\",\n",
    "    \"x-20=30,x=?\",\n",
    "    \"80-x=60,x=?\"\n",
    "]\n",
    "with torch.no_grad():\n",
    "    for prompt in test_set: \n",
    "        prompt_ids = encode(prompt)\n",
    "        ###########################################################\n",
    "        # Please complete the test code here!\n",
    "        # ...\n",
    "        # gpt.generate(x, max_new_tokens, temperature=temperature, top_k=top_k)\n",
    "        # ...\n",
    "        x = torch.tensor([prompt_ids], dtype=torch.long, device=device)  # shape [1, T]\n",
    "\n",
    "        y = gpt.generate(\n",
    "            x,\n",
    "            max_new_tokens=max_new_tokens,      # you defined these above\n",
    "            temperature=temperature,\n",
    "            top_k=top_k\n",
    "        ) \n",
    "        out_full = decode(y[0].cpu().flatten().tolist())\n",
    "        generated = out_full[len(prompt):].strip()\n",
    "\n",
    "        print(f\"Q: {prompt}\")\n",
    "        print(f\"A: {generated}\\n\")\n",
    "        ###########################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1872b7b-30d1-4ba8-83c8-19422e65b66d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (pytorch)",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
