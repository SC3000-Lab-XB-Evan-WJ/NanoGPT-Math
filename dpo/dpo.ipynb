{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "124a869a",
      "metadata": {
        "id": "124a869a"
      },
      "source": [
        "### Step 1: Install necesscary packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "id": "3b82f8f1",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3b82f8f1",
        "outputId": "059a5ab1-8849-4ac8-c834-1be72b2889ed"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (3.10.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (4.60.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.4.9)\n",
            "Requirement already satisfied: numpy>=1.23 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (3.2.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (2.9.0.post0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.8.0+cu126)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (2.0.2)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.57.1)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.12/dist-packages (4.0.0)\n",
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.12/dist-packages (0.12.0)\n",
            "Requirement already satisfied: wandb in /usr/local/lib/python3.12/dist-packages (0.22.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (4.67.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.4.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.35.3)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.4)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.6.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets) (3.6.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: click>=8.0.1 in /usr/local/lib/python3.12/dist-packages (from wandb) (8.3.0)\n",
            "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from wandb) (3.1.45)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.12/dist-packages (from wandb) (4.5.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<7,>=3.19.0 in /usr/local/lib/python3.12/dist-packages (from wandb) (5.29.5)\n",
            "Requirement already satisfied: pydantic<3 in /usr/local/lib/python3.12/dist-packages (from wandb) (2.11.10)\n",
            "Requirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from wandb) (2.42.1)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.13.1)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.12/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.12)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.1.10)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3->wandb) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3->wandb) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3->wandb) (0.4.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2025.10.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.22.0)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.12/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install matplotlib\n",
        "!pip install torch numpy transformers datasets tiktoken wandb tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "id": "454083b3-4a7c-4bc7-8b60-7bfec77c8bdd",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "454083b3-4a7c-4bc7-8b60-7bfec77c8bdd",
        "outputId": "d9df8f94-23ee-4e6b-a50e-be2613887cc0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (4.67.1)\n"
          ]
        }
      ],
      "source": [
        "!pip3 install tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "id": "1pOa2pMJcIUD",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1pOa2pMJcIUD",
        "outputId": "14a89610-bb9b-4b47-cca0-51481f2869d5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "id": "WFWX_UNMcOLS",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WFWX_UNMcOLS",
        "outputId": "cc96cd10-75c0-45ad-a192-36b1f5ad94b5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/Colab Notebooks/SC3000/dpo\n"
          ]
        }
      ],
      "source": [
        "%cd /content/drive/MyDrive/Colab Notebooks/SC3000/dpo"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6c2d9de0",
      "metadata": {
        "id": "6c2d9de0"
      },
      "source": [
        "### Step 2: Package imports and configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "id": "876dd92d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "876dd92d",
        "outputId": "61ad9f52-23fe-4600-9637-ebcb9a149fbf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cuda\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "import os\n",
        "sys.path.append(os.path.abspath(\"..\"))\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.optim import AdamW\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "import torch.nn.functional as F\n",
        "import random\n",
        "import pickle\n",
        "from model import GPT, GPTConfig\n",
        "from tqdm import tqdm\n",
        "import time\n",
        "import json\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "torch.manual_seed(42)\n",
        "random.seed(42)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed(42)\n",
        "\n",
        "\n",
        "# Configuration\n",
        "beta = 0.5\n",
        "if torch.cuda.is_available():\n",
        "    device = \"cuda\"\n",
        "    print(\"cuda\")\n",
        "elif getattr(torch.backends, \"mps\", None) and torch.backends.mps.is_available():\n",
        "    device = \"mps\"\n",
        "else:\n",
        "    device = \"cpu\"\n",
        "\n",
        "base_lr = 0.0001\n",
        "epochs = 20\n",
        "batch_size = 64\n",
        "max_length =64\n",
        "num_samples = 1\n",
        "max_new_tokens = 200\n",
        "temperature = 0.8\n",
        "top_k = 200\n",
        "# tokenizer\n",
        "with open(\"../sft/meta.pkl\", \"rb\") as f:\n",
        "    meta = pickle.load(f)\n",
        "stoi, itos = meta[\"stoi\"], meta[\"itos\"]\n",
        "#def encode(s): return [stoi[c] for c in s]\n",
        "def encode(s):\n",
        "    return [stoi[c] if c in stoi else 0 for c in s]\n",
        "def decode(l): return ''.join([itos[i] for i in l])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4c7d35e6",
      "metadata": {
        "id": "4c7d35e6"
      },
      "source": [
        "### Step 3: Define helper functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "id": "d03655c3",
      "metadata": {
        "id": "d03655c3"
      },
      "outputs": [],
      "source": [
        "def compute_logprob(input_ids):\n",
        "    inputs = input_ids[:, :-1]\n",
        "    targets = input_ids[:, 1:]\n",
        "    logits, _ = gpt(inputs, full_seq=True)\n",
        "    B, T, V = logits.size()\n",
        "    logits_flat = logits.reshape(-1, V)\n",
        "    targets_flat = targets.reshape(-1)\n",
        "    loss = F.cross_entropy(logits_flat, targets_flat, ignore_index=0, reduction='none')\n",
        "    loss = loss.reshape(B, T)\n",
        "    attention_mask = (targets != 0).float()\n",
        "    loss = (loss * attention_mask).sum(dim=1) / attention_mask.sum(dim=1)\n",
        "    return -loss\n",
        "\n",
        "def pad_or_truncate(seq, max_length):\n",
        "    return seq[-max_length:] if len(seq) > max_length else seq + [0] * (max_length - len(seq))\n",
        "\n",
        "def get_batches(lines, batch_size):\n",
        "    random.shuffle(lines)\n",
        "    #for l in lines:\n",
        "    #    print(l[1])\n",
        "    for i in range(0, len(lines), batch_size):\n",
        "        batch = lines[i:i+batch_size]\n",
        "        if len(batch) < batch_size:\n",
        "            continue\n",
        "        neg_inputs = [pad_or_truncate(encode(p['negative'] + '\\n\\n\\n\\n'), max_length) for p in batch]\n",
        "        pos_inputs = [pad_or_truncate(encode(p['positive'] + '\\n\\n\\n\\n'), max_length) for p in batch]\n",
        "        neg_tensor = torch.tensor(neg_inputs, dtype=torch.long, device=device)\n",
        "        pos_tensor = torch.tensor(pos_inputs, dtype=torch.long, device=device)\n",
        "        yield neg_tensor, pos_tensor"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fc9d9eba",
      "metadata": {
        "id": "fc9d9eba"
      },
      "source": [
        "### Step 4: Load the pretrained NanoGPT model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "id": "ceae772a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ceae772a",
        "outputId": "12880b03-8fdc-4f6d-b1f0-4be879f25e15"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "GPT(\n",
              "  (transformer): ModuleDict(\n",
              "    (wte): Embedding(74, 348)\n",
              "    (wpe): Embedding(256, 348)\n",
              "    (drop): Dropout(p=0.2, inplace=False)\n",
              "    (h): ModuleList(\n",
              "      (0-5): 6 x Block(\n",
              "        (ln_1): LayerNorm()\n",
              "        (attn): CausalSelfAttention(\n",
              "          (c_attn): Linear(in_features=348, out_features=1044, bias=False)\n",
              "          (c_proj): Linear(in_features=348, out_features=348, bias=False)\n",
              "          (attn_dropout): Dropout(p=0.2, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.2, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm()\n",
              "        (mlp): MLP(\n",
              "          (c_fc): Linear(in_features=348, out_features=1392, bias=False)\n",
              "          (gelu): GELU(approximate='none')\n",
              "          (c_proj): Linear(in_features=1392, out_features=348, bias=False)\n",
              "          (dropout): Dropout(p=0.2, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (ln_f): LayerNorm()\n",
              "  )\n",
              "  (lm_head): Linear(in_features=348, out_features=74, bias=False)\n",
              ")"
            ]
          },
          "execution_count": 39,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "ckpt = torch.load(\"../sft/gpt.pt\", map_location=device)\n",
        "gptconf = GPTConfig(**ckpt['model_args'])\n",
        "gpt = GPT(gptconf)\n",
        "state_dict = ckpt['model']\n",
        "unwanted_prefix = '_orig_mod.'\n",
        "for k in list(state_dict.keys()):\n",
        "    if k.startswith(unwanted_prefix):\n",
        "        state_dict[k[len(unwanted_prefix):]] = state_dict.pop(k)\n",
        "gpt.load_state_dict(state_dict)\n",
        "gpt.to(device).train()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0feafc5a",
      "metadata": {
        "id": "0feafc5a"
      },
      "source": [
        "### Step 5: Load Data (**students are required to complete this part!**)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7edf3d44",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7edf3d44",
        "outputId": "8de1f7ac-c8cc-4dbb-9224-7f8b7154a453"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'list'>\n",
            "750000\n",
            "{'negative': '197-2=? Sorry, I do not know!', 'positive': '197-2=? The answer is 195 because 197-2 equals 195.'}\n"
          ]
        }
      ],
      "source": [
        "# Load data from ./data/pos_neg_pairs.json\n",
        "with open('/content/drive/MyDrive/Colab Notebooks/SC3000/data/pos_neg_pairs_1mil.json', 'r', encoding='utf-8') as f:\n",
        "    data = json.load(f)\n",
        "    lines = data\n",
        "\n",
        "# Check the structure\n",
        "print(type(data))\n",
        "print(len(data))\n",
        "print(data[1])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c2e5f81f",
      "metadata": {
        "id": "c2e5f81f"
      },
      "source": [
        "### Step 6: Build the optimizer and scheduler (**students are required to complete this part!**)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "id": "df0c400f",
      "metadata": {
        "id": "df0c400f"
      },
      "outputs": [],
      "source": [
        "def build_optimizer_and_scheduler(\n",
        "    model,\n",
        "    lr=base_lr,\n",
        "    weight_decay=0.05,\n",
        "):\n",
        "    # ---- no weight decay for bias & LayerNorm/BatchNorm ----\n",
        "    decay, no_decay = [], []\n",
        "    for name, p in model.named_parameters():\n",
        "        if not p.requires_grad:\n",
        "            continue\n",
        "        if p.ndim >= 2:\n",
        "            decay.append(p)\n",
        "        else:\n",
        "            no_decay.append(p)\n",
        "\n",
        "    optim_groups = [\n",
        "        {\"params\": decay, \"weight_decay\": weight_decay},\n",
        "        {\"params\": no_decay, \"weight_decay\": 0.0},\n",
        "    ]\n",
        "\n",
        "    # AdamW optimizer\n",
        "    optimizer = AdamW(optim_groups, lr=lr)\n",
        "\n",
        "    # Step Decay scheduler\n",
        "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
        "    optimizer, T_max=len(lines) // batch_size * epochs, eta_min=1e-6\n",
        ")\n",
        "\n",
        "    return optimizer, scheduler\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "id": "d9i3yBSDAQ8q",
      "metadata": {
        "id": "d9i3yBSDAQ8q"
      },
      "outputs": [],
      "source": [
        "optimizer, scheduler = build_optimizer_and_scheduler(\n",
        "    gpt,\n",
        "    lr=base_lr,\n",
        "    weight_decay=0.05,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "52b66199",
      "metadata": {
        "id": "52b66199"
      },
      "source": [
        "### Step 7: Begin training (**students are required to complete this part!**)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "id": "1d4ebeb4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 720
        },
        "id": "1d4ebeb4",
        "outputId": "7ba7c11e-7c8e-4817-d0a5-fd1d589eb594"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "11718it [07:58, 24.46it/s, loss=0.0313, lr=9.94e-5]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved checkpoint to ./dpo.pt\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "11718it [07:58, 24.51it/s, loss=0.0271, lr=9.76e-5]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved checkpoint to ./dpo.pt\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "11718it [07:57, 24.54it/s, loss=0.0255, lr=9.46e-5]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved checkpoint to ./dpo.pt\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "11718it [07:57, 24.53it/s, loss=0.0255, lr=9.05e-5]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved checkpoint to ./dpo.pt\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "11718it [07:58, 24.50it/s, loss=0.0242, lr=8.55e-5]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved checkpoint to ./dpo.pt\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "11718it [07:57, 24.52it/s, loss=0.0244, lr=7.96e-5]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved checkpoint to ./dpo.pt\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "11718it [07:56, 24.57it/s, loss=0.0236, lr=7.3e-5]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved checkpoint to ./dpo.pt\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "11718it [07:57, 24.55it/s, loss=0.0244, lr=6.58e-5]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved checkpoint to ./dpo.pt\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "11718it [07:58, 24.49it/s, loss=0.0248, lr=5.82e-5]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved checkpoint to ./dpo.pt\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "11718it [07:57, 24.54it/s, loss=0.0245, lr=5.05e-5]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved checkpoint to ./dpo.pt\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "8815it [05:59, 24.55it/s, loss=0.0237, lr=4.47e-5]\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-150415572.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogsigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpos_logprob\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mneg_logprob\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mbeta\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m \u001b[0mpos_logprob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m0.1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset_to_none\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgpt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_norm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    645\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    646\u001b[0m             )\n\u001b[0;32m--> 647\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    648\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    649\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    352\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 354\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    355\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    827\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    828\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 829\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    830\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    831\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "gpt.to(device).train()\n",
        "\n",
        "total_steps = len(lines) // batch_size\n",
        "for epoch in range(epochs):\n",
        "    pbar = tqdm(get_batches(lines, batch_size))\n",
        "    for step, (neg_tensor,pos_tensor) in enumerate(pbar):\n",
        "        ###########################################################\n",
        "        # Please complete the training code here!\n",
        "        # Examples:\n",
        "        # ...\n",
        "        # neg_logprob\n",
        "        # pos_logprob\n",
        "        # loss = -F.logsigmoid((pos_logprob - neg_logprob) / beta).mean() - pos_logprob.mean() * 0.1\n",
        "        # ...\n",
        "        pos_logprob = compute_logprob(pos_tensor)\n",
        "        neg_logprob = compute_logprob(neg_tensor)\n",
        "        loss = -F.logsigmoid((pos_logprob - neg_logprob) / beta).mean()- pos_logprob.mean() * 0.1\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "        loss.backward()\n",
        "\n",
        "        torch.nn.utils.clip_grad_norm_(gpt.parameters(), max_norm=1.0)\n",
        "\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "\n",
        "        if (step + 1) % 10 == 0:\n",
        "            pbar.set_postfix(loss=float(loss.item()), lr=scheduler.get_last_lr()[0])\n",
        "\n",
        "        ###########################################################\n",
        "    ckpt_path = f\"./dpo.pt\"\n",
        "    torch.save({\n",
        "        \"model_state_dict\": gpt.state_dict(),\n",
        "        \"model_args\": ckpt['model_args'],\n",
        "    }, ckpt_path)\n",
        "    print(f\"Saved checkpoint to {ckpt_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "48b7f2ab",
      "metadata": {
        "id": "48b7f2ab"
      },
      "source": [
        "# Step 8: Begin testing (**students are required to complete this part!**)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "09027262",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "09027262",
        "outputId": "70f0aa54-3b94-4acc-a69d-92691bd142ef"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Q: 17+19=?\n",
            "A: The answer is 36 because 17+19 equals 36.\n",
            "\n",
            "Q: 3*17=?\n",
            "A: The answer is 51 because 3*17 equals 51.\n",
            "\n",
            "Q: 72/4=?\n",
            "A: The answer is 18 because 72/4 equals 18.\n",
            "\n",
            "Q: 72-x=34,x=?\n",
            "A: The answer is 38 because 72-34 equals 38.\n",
            "\n",
            "Q: x*11=44,x=?\n",
            "A: The answer is 44 because 44/11 equals 44.\n",
            "\n",
            "Q: 3*17=?\n",
            "A: The answer is 51 because 3*17 equals 51.\n",
            "\n",
            "Q: 72/4=?\n",
            "A: The answer is 18 because 72/4 equals 18.\n",
            "\n",
            "Q: 72-x=34,x=?\n",
            "A: The answer is 38 because 72-34 equals 38.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Load the fine-tuned model\n",
        "ckpt_path = \"/content/drive/MyDrive/Colab Notebooks/SC3000/dpo/dpo.pt\"\n",
        "checkpoint = torch.load(ckpt_path, map_location=device)\n",
        "gptconf = GPTConfig(**checkpoint['model_args'])\n",
        "gpt = GPT(gptconf).to(device)\n",
        "try:\n",
        "    state_dict = checkpoint['model']\n",
        "except:\n",
        "    state_dict = checkpoint['model_state_dict']\n",
        "unwanted_prefix = '_orig_mod.'\n",
        "for k,v in list(state_dict.items()):\n",
        "    if k.startswith(unwanted_prefix):\n",
        "        state_dict[k[len(unwanted_prefix):]] = state_dict.pop(k)\n",
        "gpt.load_state_dict(state_dict)\n",
        "# Test\n",
        "gpt.eval()\n",
        "test_set = [\"17+19=?\", \"3*17=?\", \"72/4=?\", \"72-x=34,x=?\", \"x*11=44,x=?\", \"3*17=?\", \"72/4=?\", \"72-x=34,x=?\"]\n",
        "\n",
        "with torch.no_grad():\n",
        "    for prompt in test_set:\n",
        "        prompt_ids = encode(prompt)\n",
        "        ###########################################################\n",
        "        # Please complete the test code here!\n",
        "        # ...\n",
        "        # gpt.generate(x, max_new_tokens, temperature=temperature, top_k=top_k)\n",
        "        # ...\n",
        "        x = torch.tensor([prompt_ids], dtype=torch.long, device=device)  # shape [1, T]\n",
        "\n",
        "        y = gpt.generate(\n",
        "            x,\n",
        "            max_new_tokens=max_new_tokens,      # you defined these above\n",
        "            temperature=temperature,\n",
        "            top_k=top_k\n",
        "        )\n",
        "        out_full = decode(y[0].cpu().flatten().tolist())\n",
        "        generated = out_full[len(prompt):].strip()\n",
        "\n",
        "        print(f\"Q: {prompt}\")\n",
        "        print(f\"A: {generated}\\n\")\n",
        "        ###########################################################"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
